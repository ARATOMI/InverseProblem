{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-reform",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: astropy in /opt/conda/lib/python3.8/site-packages (4.2.1)\n",
      "Requirement already satisfied: pyerfa in /opt/conda/lib/python3.8/site-packages (from astropy) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from astropy) (1.19.2)\n",
      "Requirement already satisfied: googledrivedownloader in /opt/conda/lib/python3.8/site-packages (0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install astropy\n",
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-recall",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import pickle\n",
    "import zipfile\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-admission",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin   datasets\thome   lib64   mnt\t  opt\trun   storage  usr\n",
      "boot  dev\tlib    libx32  notebooks  proc\tsbin  sys      var\n",
      "cicc  etc\tlib32  media   nvidia\t  root\tsrv   tmp      workspace\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-camping",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "Path('../storage/dataset/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-american",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mInverse\u001b[0m/  \u001b[01;34mdataset\u001b[0m/  \u001b[01;34mnotebooks\u001b[0m/  \u001b[01;34msavepoints\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-barrier",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "Path('../storage/dataset/').mkdir(parents=True, exist_ok=True)\n",
    "filepath = '../storage/dataset/parameters_base.fits'\n",
    "if not os.path.isfile(filepath):\n",
    "    print('Download from google disk')\n",
    "    parameters_link = \"https://drive.google.com/file/d/10HzzxlPoBnmqJb_v2NCyaUwO0ct6JD71/view?usp=sharing\"\n",
    "    fileid = '10HzzxlPoBnmqJb_v2NCyaUwO0ct6JD71'\n",
    "    dest_path = '../storage/dataset/parameters_base.fits.zip'\n",
    "    gdd.download_file_from_google_drive(file_id=fileid,\n",
    "                                   dest_path=dest_path, showsize=True)\n",
    "    with zipfile.ZipFile('../storage/dataset/parameters_base.fits.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('../storage/dataset/')\n",
    "    os.remove('../storage/dataset/parameters_base.fits.zip')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-pathology",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1oafzBpvf0ALFTVol6xzW6hW3RcLPXN2x into ../storage/dataset/20170905_030404.fits.zip... \n",
      "27.8 MiB        Done.\n"
     ]
    }
   ],
   "source": [
    "fileid = '1oafzBpvf0ALFTVol6xzW6hW3RcLPXN2x'\n",
    "dest_path = '../storage/dataset/20170905_030404.fits.zip'\n",
    "gdd.download_file_from_google_drive(file_id=fileid,\n",
    "                                dest_path=dest_path, showsize=True)\n",
    "with zipfile.ZipFile('../storage/dataset/20170905_030404.fits.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('../storage/dataset/')\n",
    "os.remove('../storage/dataset/20170905_030404.fits.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-egyptian",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170905_030404.fits   brand_b6000d100.tar.gz  prediction_ind.fits\n",
      "\u001b[0m\u001b[01;34m__MACOSX\u001b[0m/              gdrive.sh\n",
      "brand_b100d100.tar.gz  parameters_base.fits\n"
     ]
    }
   ],
   "source": [
    "ls ../storage/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-acrobat",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'InverseProblem'...\n",
      "remote: Enumerating objects: 592, done.\u001b[K\n",
      "remote: Counting objects: 100% (592/592), done.\u001b[K\n",
      "remote: Compressing objects: 100% (353/353), done.\u001b[K\n",
      "remote: Total 592 (delta 428), reused 391 (delta 236), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (592/592), 2.65 MiB | 1.58 MiB/s, done.\n",
      "Resolving deltas: 100% (428/428), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/iknyazeva/InverseProblem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-chile",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('InverseProblem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-cornell",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from inverse_problem.nn_inversion.main import HyperParams, Model\n",
    "from inverse_problem.nn_inversion.transforms import normalize_output\n",
    "from inverse_problem.nn_inversion.posthoc import compute_metrics, open_param_file, plot_params\n",
    "from astropy.io import fits\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-skiing",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "params = fits.open('../storage/dataset/parameters_base.fits')[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-circle",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.20757286e+02, 8.65463943e+01, 1.21179359e+02, 2.79353294e+01,\n",
       "       1.62066445e-01, 1.92586861e+01, 2.72371914e+04, 1.62790508e+04,\n",
       "       6.21470988e-01, 4.35866445e-01, 2.30690539e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-caribbean",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HyperParams(hps_name='hps_independent_mlp', n_input=224, batch_norm=True, dropout=0.05, hidden_dims=[100, 100, 100], bottom_output=224, predict_ind=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], activation='elu', val_split=0.05, top_output=11, top_layers=2, transform_type='mlp_transform_rescale', mode='range', logB=True, factors=[1, 1000, 1000, 1000], cont_scale=40000, norm_output=True, source='database', hidden_size=100, bn=1, top_net='TopIndependentNet', bottom_net='ZeroMLP', lr=0.0001, lr_decay=0.005, weight_decay=0.0, batch_size=128, n_epochs=5, trainset=None, valset=None, patience=1, absolute_noise_levels=[109, 28, 28, 44])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_json =  \"InverseProblem/res_experiments/hps_independent_mlp.json\"\n",
    "hps = HyperParams.from_file(path_to_json=path_to_json)\n",
    "hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-storm",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "model = Model(hps)\n",
    "#model.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-construction",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "Path('../storage/Inverse/trainlogs/').mkdir(parents=True, exist_ok=True)\n",
    "Path('../storage/Inverse/trained_models/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-telescope",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_models\ttrainlogs\n"
     ]
    }
   ],
   "source": [
    "!ls ../storage/Inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-elder",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-glory",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "batch:   0%|          | 0/24032 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   0%|          | 100/24032 [00:24<1:38:50,  4.04it/s]\u001b[A\n",
      "batch:   1%|          | 200/24032 [00:43<1:31:03,  4.36it/s]\u001b[A\n",
      "batch:   1%|          | 300/24032 [01:01<1:24:31,  4.68it/s]\u001b[A\n",
      "batch:   2%|▏         | 400/24032 [01:19<1:20:31,  4.89it/s]\u001b[A\n",
      "batch:   2%|▏         | 500/24032 [01:37<1:17:05,  5.09it/s]\u001b[A\n",
      "batch:   2%|▏         | 600/24032 [01:54<1:14:22,  5.25it/s]\u001b[A\n",
      "batch:   3%|▎         | 700/24032 [02:12<1:12:35,  5.36it/s]\u001b[A\n",
      "batch:   3%|▎         | 800/24032 [02:30<1:11:21,  5.43it/s]\u001b[A\n",
      "batch:   4%|▎         | 900/24032 [02:48<1:11:06,  5.42it/s]\u001b[A\n",
      "batch:   4%|▍         | 1000/24032 [03:06<1:10:06,  5.48it/s]\u001b[A\n",
      "batch:   5%|▍         | 1100/24032 [03:24<1:09:30,  5.50it/s]\u001b[A\n",
      "batch:   5%|▍         | 1200/24032 [03:42<1:09:04,  5.51it/s]\u001b[A\n",
      "batch:   5%|▌         | 1300/24032 [04:03<1:11:54,  5.27it/s]\u001b[A\n",
      "batch:   6%|▌         | 1400/24032 [04:24<1:13:13,  5.15it/s]\u001b[A\n",
      "batch:   6%|▌         | 1500/24032 [04:44<1:13:23,  5.12it/s]\u001b[A\n",
      "batch:   7%|▋         | 1600/24032 [05:02<1:11:29,  5.23it/s]\u001b[A\n",
      "batch:   7%|▋         | 1700/24032 [05:20<1:09:53,  5.33it/s]\u001b[A\n",
      "batch:   7%|▋         | 1800/24032 [05:38<1:08:52,  5.38it/s]\u001b[A\n",
      "batch:   8%|▊         | 1900/24032 [05:56<1:07:48,  5.44it/s]\u001b[A\n",
      "batch:   8%|▊         | 2000/24032 [06:14<1:07:13,  5.46it/s]\u001b[A\n",
      "batch:   9%|▊         | 2100/24032 [06:32<1:06:24,  5.50it/s]\u001b[A\n",
      "batch:   9%|▉         | 2200/24032 [06:50<1:06:00,  5.51it/s]\u001b[A\n",
      "batch:  10%|▉         | 2300/24032 [07:08<1:06:15,  5.47it/s]\u001b[A\n",
      "batch:  10%|▉         | 2400/24032 [07:28<1:07:12,  5.36it/s]\u001b[A\n",
      "batch:  10%|█         | 2500/24032 [07:47<1:07:42,  5.30it/s]\u001b[A\n",
      "batch:  11%|█         | 2600/24032 [08:09<1:10:31,  5.06it/s]\u001b[A\n",
      "batch:  11%|█         | 2700/24032 [08:29<1:10:18,  5.06it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2800/24032 [08:50<1:11:14,  4.97it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2900/24032 [09:10<1:10:48,  4.97it/s]\u001b[A\n",
      "batch:  12%|█▏        | 3000/24032 [09:32<1:12:03,  4.86it/s]\u001b[A\n",
      "batch:  13%|█▎        | 3100/24032 [09:51<1:10:13,  4.97it/s]\u001b[A\n",
      "batch:  13%|█▎        | 3200/24032 [10:11<1:10:01,  4.96it/s]\u001b[A\n",
      "batch:  14%|█▎        | 3300/24032 [10:30<1:08:58,  5.01it/s]\u001b[A\n",
      "batch:  14%|█▍        | 3400/24032 [10:48<1:06:37,  5.16it/s]\u001b[A\n",
      "batch:  15%|█▍        | 3500/24032 [11:07<1:05:02,  5.26it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4000/24032 [12:43<1:03:37,  5.25it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4100/24032 [13:02<1:03:03,  5.27it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4200/24032 [13:23<1:04:03,  5.16it/s]\u001b[A\n",
      "batch:  18%|█▊        | 4300/24032 [13:42<1:03:23,  5.19it/s]\u001b[A\n",
      "batch:  18%|█▊        | 4400/24032 [14:03<1:04:43,  5.06it/s]\u001b[A\n",
      "batch:  19%|█▊        | 4500/24032 [14:22<1:03:42,  5.11it/s]\u001b[A\n",
      "batch:  19%|█▉        | 4600/24032 [14:40<1:02:34,  5.18it/s]\u001b[A\n",
      "batch:  20%|█▉        | 4700/24032 [15:00<1:02:26,  5.16it/s]\u001b[A\n",
      "batch:  20%|█▉        | 4800/24032 [15:18<1:01:12,  5.24it/s]\u001b[A\n",
      "batch:  20%|██        | 4900/24032 [15:36<1:00:00,  5.31it/s]\u001b[A\n",
      "batch:  21%|██        | 5000/24032 [15:55<59:08,  5.36it/s]  \u001b[A\n",
      "batch:  21%|██        | 5100/24032 [16:12<58:00,  5.44it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5200/24032 [16:31<57:31,  5.46it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5300/24032 [16:49<57:09,  5.46it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5400/24032 [17:07<56:16,  5.52it/s]\u001b[A\n",
      "batch:  23%|██▎       | 5500/24032 [17:25<55:48,  5.54it/s]\u001b[A\n",
      "batch:  23%|██▎       | 5600/24032 [17:42<55:21,  5.55it/s]\u001b[A\n",
      "batch:  24%|██▎       | 5700/24032 [18:01<55:28,  5.51it/s]\u001b[A\n",
      "batch:  24%|██▍       | 5800/24032 [18:22<57:36,  5.27it/s]\u001b[A\n",
      "batch:  25%|██▍       | 5900/24032 [18:43<59:00,  5.12it/s]\u001b[A\n",
      "batch:  25%|██▍       | 6000/24032 [19:04<1:00:11,  4.99it/s]\u001b[A\n",
      "batch:  25%|██▌       | 6100/24032 [19:25<1:00:30,  4.94it/s]\u001b[A\n",
      "batch:  26%|██▌       | 6200/24032 [19:44<59:27,  5.00it/s]  \u001b[A\n",
      "batch:  26%|██▌       | 6300/24032 [20:02<57:30,  5.14it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6400/24032 [20:20<56:07,  5.24it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6500/24032 [20:39<55:20,  5.28it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6600/24032 [20:57<54:12,  5.36it/s]\u001b[A\n",
      "batch:  28%|██▊       | 6700/24032 [21:15<53:21,  5.41it/s]\u001b[A\n",
      "batch:  28%|██▊       | 6800/24032 [21:33<52:39,  5.45it/s]\u001b[A\n",
      "batch:  29%|██▊       | 6900/24032 [21:51<51:52,  5.50it/s]\u001b[A\n",
      "batch:  29%|██▉       | 7000/24032 [22:10<52:09,  5.44it/s]\u001b[A\n",
      "batch:  30%|██▉       | 7100/24032 [22:29<52:33,  5.37it/s]\u001b[A\n",
      "batch:  30%|██▉       | 7200/24032 [22:47<51:30,  5.45it/s]\u001b[A\n",
      "batch:  30%|███       | 7300/24032 [23:05<50:56,  5.47it/s]\u001b[A\n",
      "batch:  31%|███       | 7400/24032 [23:22<49:55,  5.55it/s]\u001b[A\n",
      "batch:  31%|███       | 7500/24032 [23:40<49:27,  5.57it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7600/24032 [23:58<49:22,  5.55it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7700/24032 [24:17<49:26,  5.51it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7800/24032 [24:37<50:32,  5.35it/s]\u001b[A\n",
      "batch:  33%|███▎      | 7900/24032 [24:55<50:23,  5.34it/s]\u001b[A\n",
      "batch:  33%|███▎      | 8000/24032 [25:16<51:28,  5.19it/s]\u001b[A\n",
      "batch:  34%|███▎      | 8100/24032 [25:37<52:29,  5.06it/s]\u001b[A\n",
      "batch:  34%|███▍      | 8200/24032 [25:56<51:16,  5.15it/s]\u001b[A\n",
      "batch:  35%|███▍      | 8300/24032 [26:14<50:21,  5.21it/s]\u001b[A\n",
      "batch:  35%|███▍      | 8400/24032 [26:33<49:25,  5.27it/s]\u001b[A\n",
      "batch:  35%|███▌      | 8500/24032 [26:51<48:19,  5.36it/s]\u001b[A\n",
      "batch:  36%|███▌      | 8600/24032 [27:09<47:47,  5.38it/s]\u001b[A\n",
      "batch:  36%|███▌      | 8700/24032 [27:27<47:01,  5.43it/s]\u001b[A\n",
      "batch:  37%|███▋      | 8800/24032 [27:45<46:40,  5.44it/s]\u001b[A\n",
      "batch:  37%|███▋      | 8900/24032 [28:05<47:13,  5.34it/s]\u001b[A\n",
      "batch:  37%|███▋      | 9000/24032 [28:25<48:22,  5.18it/s]\u001b[A\n",
      "batch:  38%|███▊      | 9100/24032 [28:43<47:02,  5.29it/s]\u001b[A\n",
      "batch:  38%|███▊      | 9200/24032 [29:02<46:21,  5.33it/s]\u001b[A\n",
      "batch:  39%|███▊      | 9300/24032 [29:20<45:19,  5.42it/s]\u001b[A\n",
      "batch:  39%|███▉      | 9400/24032 [29:38<44:49,  5.44it/s]\u001b[A\n",
      "batch:  40%|███▉      | 9500/24032 [29:57<44:54,  5.39it/s]\u001b[A\n",
      "batch:  40%|███▉      | 9600/24032 [30:16<44:48,  5.37it/s]\u001b[A\n",
      "batch:  40%|████      | 9700/24032 [30:37<46:14,  5.17it/s]\u001b[A\n",
      "batch:  41%|████      | 9800/24032 [30:56<45:48,  5.18it/s]\u001b[A\n",
      "batch:  41%|████      | 9900/24032 [31:16<45:53,  5.13it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10000/24032 [31:35<45:33,  5.13it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10100/24032 [31:53<44:14,  5.25it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10200/24032 [32:11<43:12,  5.34it/s]\u001b[A\n",
      "batch:  43%|████▎     | 10300/24032 [32:29<42:25,  5.40it/s]\u001b[A\n",
      "batch:  43%|████▎     | 10400/24032 [32:47<41:53,  5.42it/s]\u001b[A\n",
      "batch:  44%|████▎     | 10500/24032 [33:06<41:43,  5.41it/s]\u001b[A\n",
      "batch:  44%|████▍     | 10600/24032 [33:27<42:52,  5.22it/s]\u001b[A\n",
      "batch:  45%|████▍     | 10700/24032 [33:48<43:49,  5.07it/s]\u001b[A\n",
      "batch:  45%|████▍     | 10800/24032 [34:08<43:29,  5.07it/s]\u001b[A\n",
      "batch:  45%|████▌     | 10900/24032 [34:27<43:09,  5.07it/s]\u001b[A\n",
      "batch:  46%|████▌     | 11000/24032 [34:46<42:23,  5.12it/s]\u001b[A\n",
      "batch:  46%|████▌     | 11100/24032 [35:05<41:48,  5.15it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11200/24032 [35:27<42:39,  5.01it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11300/24032 [35:47<42:24,  5.00it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11400/24032 [36:06<41:33,  5.07it/s]\u001b[A\n",
      "batch:  48%|████▊     | 11500/24032 [36:24<40:27,  5.16it/s]\u001b[A\n",
      "batch:  48%|████▊     | 11600/24032 [36:43<39:24,  5.26it/s]\u001b[A\n",
      "batch:  49%|████▊     | 11700/24032 [37:02<39:26,  5.21it/s]\u001b[A\n",
      "batch:  49%|████▉     | 11800/24032 [37:25<41:14,  4.94it/s]\u001b[A\n",
      "batch:  50%|████▉     | 11900/24032 [37:46<41:11,  4.91it/s]\u001b[A\n",
      "batch:  50%|████▉     | 12000/24032 [38:04<39:36,  5.06it/s]\u001b[A\n",
      "batch:  50%|█████     | 12100/24032 [38:24<39:11,  5.07it/s]\u001b[A\n",
      "batch:  51%|█████     | 12200/24032 [38:44<39:08,  5.04it/s]\u001b[A\n",
      "batch:  51%|█████     | 12300/24032 [39:05<39:57,  4.89it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12400/24032 [39:26<39:55,  4.86it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12500/24032 [39:46<38:47,  4.96it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12600/24032 [40:05<37:45,  5.05it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 12700/24032 [40:23<36:40,  5.15it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 12800/24032 [40:43<36:46,  5.09it/s]\u001b[A\n",
      "batch:  54%|█████▎    | 12900/24032 [41:05<37:27,  4.95it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 13000/24032 [41:26<37:45,  4.87it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 13100/24032 [41:47<37:42,  4.83it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 13200/24032 [42:06<36:32,  4.94it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13600/24032 [43:28<35:29,  4.90it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13700/24032 [43:50<36:01,  4.78it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13800/24032 [44:12<36:11,  4.71it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 13900/24032 [44:35<36:27,  4.63it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 14000/24032 [44:54<35:12,  4.75it/s]\u001b[A\n",
      "batch:  59%|█████▊    | 14100/24032 [45:13<33:41,  4.91it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 14200/24032 [45:32<32:28,  5.05it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 14300/24032 [45:50<31:26,  5.16it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 14400/24032 [46:09<30:48,  5.21it/s]\u001b[A\n",
      "batch:  60%|██████    | 14500/24032 [46:27<30:11,  5.26it/s]\u001b[A\n",
      "batch:  61%|██████    | 14600/24032 [46:46<29:39,  5.30it/s]\u001b[A\n",
      "batch:  61%|██████    | 14700/24032 [47:05<29:36,  5.25it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 14800/24032 [47:26<30:07,  5.11it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 14900/24032 [47:45<29:20,  5.19it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 15000/24032 [48:03<28:43,  5.24it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 15100/24032 [48:22<28:13,  5.28it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 15200/24032 [48:40<27:34,  5.34it/s]\u001b[A\n",
      "batch:  64%|██████▎   | 15300/24032 [48:59<27:18,  5.33it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 15400/24032 [49:17<26:48,  5.37it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 15500/24032 [49:36<26:26,  5.38it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 15600/24032 [49:54<26:06,  5.38it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 15700/24032 [50:14<26:10,  5.30it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 15800/24032 [50:32<25:43,  5.33it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 15900/24032 [50:52<25:38,  5.28it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16000/24032 [51:12<25:55,  5.16it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16100/24032 [51:34<26:31,  4.98it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16200/24032 [51:57<27:19,  4.78it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 16300/24032 [52:21<28:21,  4.54it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 16400/24032 [52:42<27:17,  4.66it/s]\u001b[A\n",
      "batch:  69%|██████▊   | 16500/24032 [53:00<25:58,  4.83it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 16600/24032 [53:19<24:47,  5.00it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 16700/24032 [53:38<23:57,  5.10it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 16800/24032 [53:56<23:18,  5.17it/s]\u001b[A\n",
      "batch:  70%|███████   | 16900/24032 [54:15<22:36,  5.26it/s]\u001b[A\n",
      "batch:  71%|███████   | 17000/24032 [54:34<22:24,  5.23it/s]\u001b[A\n",
      "batch:  71%|███████   | 17100/24032 [54:54<22:23,  5.16it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17200/24032 [55:13<22:08,  5.14it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17300/24032 [55:33<21:43,  5.17it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17400/24032 [55:53<21:46,  5.08it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 17500/24032 [56:12<21:19,  5.10it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 17600/24032 [56:32<20:55,  5.12it/s]\u001b[A\n",
      "batch:  74%|███████▎  | 17700/24032 [56:50<20:19,  5.19it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 17800/24032 [57:09<19:48,  5.24it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 17900/24032 [57:27<19:15,  5.31it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 18000/24032 [57:46<18:52,  5.33it/s]\u001b[A\n",
      "batch:  75%|███████▌  | 18100/24032 [58:04<18:22,  5.38it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 18200/24032 [58:22<17:58,  5.41it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 18300/24032 [58:41<17:33,  5.44it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18400/24032 [58:58<17:06,  5.49it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18500/24032 [59:17<16:48,  5.48it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18600/24032 [59:35<16:31,  5.48it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 18700/24032 [59:54<16:24,  5.42it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 18800/24032 [1:00:12<16:00,  5.45it/s]\u001b[A\n",
      "batch:  79%|███████▊  | 18900/24032 [1:00:30<15:34,  5.49it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 19000/24032 [1:00:48<15:14,  5.50it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 19100/24032 [1:01:06<14:54,  5.51it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 19200/24032 [1:01:24<14:37,  5.51it/s]\u001b[A\n",
      "batch:  80%|████████  | 19300/24032 [1:01:42<14:20,  5.50it/s]\u001b[A\n",
      "batch:  81%|████████  | 19400/24032 [1:02:00<13:59,  5.52it/s]\u001b[A\n",
      "batch:  81%|████████  | 19500/24032 [1:02:19<13:44,  5.50it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19600/24032 [1:02:37<13:30,  5.47it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19700/24032 [1:02:56<13:15,  5.45it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19800/24032 [1:03:15<13:01,  5.42it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 19900/24032 [1:03:33<12:42,  5.42it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 20000/24032 [1:03:51<12:20,  5.44it/s]\u001b[A\n",
      "batch:  84%|████████▎ | 20100/24032 [1:04:09<11:57,  5.48it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 20200/24032 [1:04:27<11:37,  5.50it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 20300/24032 [1:04:45<11:16,  5.52it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 20400/24032 [1:05:03<11:00,  5.50it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 20500/24032 [1:05:22<10:46,  5.47it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 20600/24032 [1:05:40<10:27,  5.47it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 20700/24032 [1:05:58<10:08,  5.48it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 20800/24032 [1:06:17<09:49,  5.49it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 20900/24032 [1:06:35<09:30,  5.49it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 21000/24032 [1:06:53<09:11,  5.50it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 21100/24032 [1:07:11<08:51,  5.52it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 21200/24032 [1:07:29<08:35,  5.49it/s]\u001b[A\n",
      "batch:  89%|████████▊ | 21300/24032 [1:07:48<08:23,  5.43it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 21400/24032 [1:08:06<08:02,  5.46it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 21500/24032 [1:08:25<07:44,  5.45it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 21600/24032 [1:08:44<07:31,  5.38it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22100/24032 [1:10:22<06:21,  5.07it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22200/24032 [1:10:42<06:02,  5.05it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 22300/24032 [1:11:01<05:37,  5.13it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 22400/24032 [1:11:20<05:15,  5.16it/s]\u001b[A\n",
      "batch:  94%|█████████▎| 22500/24032 [1:11:39<04:55,  5.19it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 22600/24032 [1:11:59<04:42,  5.07it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 22700/24032 [1:12:18<04:19,  5.13it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 22800/24032 [1:12:37<03:56,  5.21it/s]\u001b[A\n",
      "batch:  95%|█████████▌| 22900/24032 [1:12:57<03:39,  5.15it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 23000/24032 [1:13:19<03:28,  4.95it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 23100/24032 [1:13:39<03:09,  4.93it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 23200/24032 [1:14:01<02:52,  4.83it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 23300/24032 [1:14:21<02:29,  4.91it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 23400/24032 [1:14:40<02:06,  5.00it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 23500/24032 [1:14:59<01:44,  5.09it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 23600/24032 [1:15:17<01:23,  5.19it/s]\u001b[A\n",
      "batch:  99%|█████████▊| 23700/24032 [1:15:36<01:03,  5.23it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 23800/24032 [1:15:55<00:44,  5.21it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 23900/24032 [1:16:14<00:25,  5.25it/s]\u001b[A\n",
      "batch: 100%|█████████▉| 24000/24032 [1:16:38<00:06,  5.22it/s]\u001b[A\n",
      "epoch:  20%|██        | 1/5 [1:19:33<5:18:13, 4773.32s/it]\n",
      "batch:   0%|          | 0/24032 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 0.1521          val_loss 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "batch:   0%|          | 100/24032 [00:17<1:08:54,  5.79it/s]\u001b[A\n",
      "batch:   1%|          | 200/24032 [00:34<1:08:19,  5.81it/s]\u001b[A\n",
      "batch:   1%|          | 300/24032 [00:51<1:07:47,  5.84it/s]\u001b[A\n",
      "batch:   2%|▏         | 400/24032 [01:08<1:07:50,  5.81it/s]\u001b[A\n",
      "batch:   2%|▏         | 500/24032 [01:26<1:08:35,  5.72it/s]\u001b[A\n",
      "batch:   2%|▏         | 600/24032 [01:46<1:11:23,  5.47it/s]\u001b[A\n",
      "batch:   3%|▎         | 700/24032 [02:06<1:13:04,  5.32it/s]\u001b[A\n",
      "batch:   3%|▎         | 800/24032 [02:25<1:12:20,  5.35it/s]\u001b[A\n",
      "batch:   4%|▎         | 900/24032 [02:43<1:11:33,  5.39it/s]\u001b[A\n",
      "batch:   4%|▍         | 1000/24032 [03:01<1:10:51,  5.42it/s]\u001b[A\n",
      "batch:   5%|▍         | 1100/24032 [03:19<1:09:14,  5.52it/s]\u001b[A\n",
      "batch:   5%|▍         | 1200/24032 [03:36<1:07:38,  5.63it/s]\u001b[A\n",
      "batch:   5%|▌         | 1300/24032 [03:53<1:06:37,  5.69it/s]\u001b[A\n",
      "batch:   6%|▌         | 1400/24032 [04:10<1:05:49,  5.73it/s]\u001b[A\n",
      "batch:   6%|▌         | 1500/24032 [04:27<1:05:01,  5.77it/s]\u001b[A\n",
      "batch:   7%|▋         | 1600/24032 [04:44<1:04:29,  5.80it/s]\u001b[A\n",
      "batch:   7%|▋         | 1700/24032 [05:01<1:04:20,  5.78it/s]\u001b[A\n",
      "batch:   7%|▋         | 1800/24032 [05:21<1:06:48,  5.55it/s]\u001b[A\n",
      "batch:   8%|▊         | 1900/24032 [05:39<1:06:27,  5.55it/s]\u001b[A\n",
      "batch:   8%|▊         | 2000/24032 [05:57<1:05:30,  5.61it/s]\u001b[A\n",
      "batch:   9%|▊         | 2100/24032 [06:14<1:04:26,  5.67it/s]\u001b[A\n",
      "batch:   9%|▉         | 2200/24032 [06:31<1:03:40,  5.71it/s]\u001b[A\n",
      "batch:  10%|▉         | 2300/24032 [06:48<1:02:38,  5.78it/s]\u001b[A\n",
      "batch:  10%|▉         | 2400/24032 [07:05<1:02:00,  5.81it/s]\u001b[A\n",
      "batch:  10%|█         | 2500/24032 [07:23<1:02:44,  5.72it/s]\u001b[A\n",
      "batch:  11%|█         | 2600/24032 [07:41<1:02:39,  5.70it/s]\u001b[A\n",
      "batch:  11%|█         | 2700/24032 [07:58<1:02:07,  5.72it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2800/24032 [08:15<1:01:19,  5.77it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2900/24032 [08:32<1:00:41,  5.80it/s]\u001b[A\n",
      "batch:  12%|█▏        | 3000/24032 [08:49<1:00:22,  5.81it/s]\u001b[A\n",
      "batch:  13%|█▎        | 3100/24032 [09:06<59:35,  5.86it/s]  \u001b[A\n",
      "batch:  13%|█▎        | 3200/24032 [09:24<1:00:19,  5.76it/s]\u001b[A\n",
      "batch:  14%|█▎        | 3300/24032 [09:42<1:00:46,  5.69it/s]\u001b[A\n",
      "batch:  14%|█▍        | 3400/24032 [10:00<1:01:27,  5.60it/s]\u001b[A\n",
      "batch:  15%|█▍        | 3500/24032 [10:20<1:02:43,  5.46it/s]\u001b[A\n",
      "batch:  15%|█▍        | 3600/24032 [10:41<1:05:19,  5.21it/s]\u001b[A\n",
      "batch:  15%|█▌        | 3700/24032 [10:59<1:03:51,  5.31it/s]\u001b[A\n",
      "batch:  16%|█▌        | 3800/24032 [11:16<1:01:59,  5.44it/s]\u001b[A\n",
      "batch:  16%|█▌        | 3900/24032 [11:33<1:00:17,  5.57it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4000/24032 [11:51<59:17,  5.63it/s]  \u001b[A\n",
      "batch:  17%|█▋        | 4100/24032 [12:07<58:01,  5.72it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4200/24032 [12:25<57:19,  5.77it/s]\u001b[A\n",
      "batch:  18%|█▊        | 4300/24032 [12:42<56:54,  5.78it/s]\u001b[A\n",
      "batch:  18%|█▊        | 4400/24032 [12:59<56:10,  5.83it/s]\u001b[A\n",
      "batch:  19%|█▊        | 4500/24032 [13:16<55:52,  5.83it/s]\u001b[A\n",
      "batch:  19%|█▉        | 4600/24032 [13:33<55:25,  5.84it/s]\u001b[A\n",
      "batch:  20%|█▉        | 4700/24032 [13:51<56:04,  5.75it/s]\u001b[A\n",
      "batch:  20%|█▉        | 4800/24032 [14:08<55:42,  5.75it/s]\u001b[A\n",
      "batch:  20%|██        | 4900/24032 [14:26<55:28,  5.75it/s]\u001b[A\n",
      "batch:  21%|██        | 5000/24032 [14:43<55:27,  5.72it/s]\u001b[A\n",
      "batch:  21%|██        | 5100/24032 [15:01<55:42,  5.66it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5200/24032 [15:21<57:42,  5.44it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5300/24032 [15:40<57:46,  5.40it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5400/24032 [16:01<59:27,  5.22it/s]\u001b[A\n",
      "batch:  23%|██▎       | 5500/24032 [16:22<1:00:35,  5.10it/s]\u001b[A\n",
      "batch:  23%|██▎       | 5600/24032 [16:42<1:00:47,  5.05it/s]\u001b[A\n",
      "batch:  24%|██▎       | 5700/24032 [17:00<59:06,  5.17it/s]  \u001b[A\n",
      "batch:  24%|██▍       | 5800/24032 [17:17<56:45,  5.35it/s]\u001b[A\n",
      "batch:  25%|██▍       | 5900/24032 [17:34<54:52,  5.51it/s]\u001b[A\n",
      "batch:  25%|██▍       | 6000/24032 [17:52<53:56,  5.57it/s]\u001b[A\n",
      "batch:  25%|██▌       | 6100/24032 [18:09<52:56,  5.64it/s]\u001b[A\n",
      "batch:  26%|██▌       | 6200/24032 [18:26<52:08,  5.70it/s]\u001b[A\n",
      "batch:  26%|██▌       | 6300/24032 [18:43<51:48,  5.70it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6400/24032 [19:03<53:19,  5.51it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6500/24032 [19:23<54:31,  5.36it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6600/24032 [19:43<55:26,  5.24it/s]\u001b[A\n",
      "batch:  28%|██▊       | 6700/24032 [20:03<56:08,  5.15it/s]\u001b[A\n",
      "batch:  28%|██▊       | 6800/24032 [20:24<56:48,  5.06it/s]\u001b[A\n",
      "batch:  29%|██▊       | 6900/24032 [20:43<56:22,  5.07it/s]\u001b[A\n",
      "batch:  29%|██▉       | 7000/24032 [21:01<54:05,  5.25it/s]\u001b[A\n",
      "batch:  30%|██▉       | 7100/24032 [21:18<52:15,  5.40it/s]\u001b[A\n",
      "batch:  30%|██▉       | 7200/24032 [21:35<50:54,  5.51it/s]\u001b[A\n",
      "batch:  30%|███       | 7300/24032 [21:53<50:08,  5.56it/s]\u001b[A\n",
      "batch:  31%|███       | 7400/24032 [22:13<51:12,  5.41it/s]\u001b[A\n",
      "batch:  31%|███       | 7500/24032 [22:33<52:23,  5.26it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7600/24032 [22:51<50:56,  5.38it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7700/24032 [23:08<49:55,  5.45it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7800/24032 [23:26<48:57,  5.53it/s]\u001b[A\n",
      "batch:  33%|███▎      | 7900/24032 [23:43<48:10,  5.58it/s]\u001b[A\n",
      "batch:  33%|███▎      | 8000/24032 [24:02<48:10,  5.55it/s]\u001b[A\n",
      "batch:  34%|███▎      | 8100/24032 [24:21<48:42,  5.45it/s]\u001b[A\n",
      "batch:  34%|███▍      | 8200/24032 [24:38<47:51,  5.51it/s]\u001b[A\n",
      "batch:  35%|███▍      | 8300/24032 [24:56<47:11,  5.56it/s]\u001b[A\n",
      "batch:  35%|███▍      | 8400/24032 [25:13<46:24,  5.61it/s]\u001b[A\n",
      "batch:  35%|███▌      | 8500/24032 [25:31<46:00,  5.63it/s]\u001b[A\n",
      "batch:  36%|███▌      | 8600/24032 [25:49<45:33,  5.65it/s]\u001b[A\n",
      "batch:  36%|███▌      | 8700/24032 [26:06<44:57,  5.68it/s]\u001b[A\n",
      "batch:  37%|███▋      | 8800/24032 [26:23<44:24,  5.72it/s]\u001b[A\n",
      "batch:  37%|███▋      | 8900/24032 [26:42<44:47,  5.63it/s]\u001b[A\n",
      "batch:  37%|███▋      | 9000/24032 [27:01<45:31,  5.50it/s]\u001b[A\n",
      "batch:  38%|███▊      | 9100/24032 [27:20<45:40,  5.45it/s]\u001b[A\n",
      "batch:  38%|███▊      | 9200/24032 [27:38<45:17,  5.46it/s]\u001b[A\n",
      "batch:  39%|███▊      | 9300/24032 [27:56<44:35,  5.51it/s]\u001b[A\n",
      "batch:  39%|███▉      | 9400/24032 [28:13<43:42,  5.58it/s]\u001b[A\n",
      "batch:  40%|███▉      | 9500/24032 [28:30<43:04,  5.62it/s]\u001b[A\n",
      "batch:  40%|███▉      | 9600/24032 [28:48<42:27,  5.66it/s]\u001b[A\n",
      "batch:  40%|████      | 9700/24032 [29:05<41:56,  5.69it/s]\u001b[A\n",
      "batch:  41%|████      | 9800/24032 [29:22<41:12,  5.76it/s]\u001b[A\n",
      "batch:  41%|████      | 9900/24032 [29:40<41:01,  5.74it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10000/24032 [29:59<42:06,  5.55it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10100/24032 [30:19<42:59,  5.40it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10200/24032 [30:37<42:21,  5.44it/s]\u001b[A\n",
      "batch:  43%|████▎     | 10300/24032 [30:56<42:26,  5.39it/s]\u001b[A\n",
      "batch:  43%|████▎     | 10400/24032 [31:14<42:15,  5.38it/s]\u001b[A\n",
      "batch:  44%|████▎     | 10500/24032 [31:33<42:09,  5.35it/s]\u001b[A\n",
      "batch:  44%|████▍     | 10600/24032 [31:51<40:59,  5.46it/s]\u001b[A\n",
      "batch:  45%|████▍     | 10700/24032 [32:09<40:29,  5.49it/s]\u001b[A\n",
      "batch:  45%|████▍     | 10800/24032 [32:27<40:03,  5.51it/s]\u001b[A\n",
      "batch:  45%|████▌     | 10900/24032 [32:45<39:40,  5.52it/s]\u001b[A\n",
      "batch:  46%|████▌     | 11000/24032 [33:03<39:32,  5.49it/s]\u001b[A\n",
      "batch:  46%|████▌     | 11100/24032 [33:22<39:30,  5.46it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11200/24032 [33:40<38:51,  5.50it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11300/24032 [33:58<38:35,  5.50it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11400/24032 [34:15<37:53,  5.56it/s]\u001b[A\n",
      "batch:  48%|████▊     | 11500/24032 [34:33<37:30,  5.57it/s]\u001b[A\n",
      "batch:  48%|████▊     | 11600/24032 [34:51<37:22,  5.54it/s]\u001b[A\n",
      "batch:  49%|████▊     | 11700/24032 [35:09<36:45,  5.59it/s]\u001b[A\n",
      "batch:  49%|████▉     | 11800/24032 [35:27<36:20,  5.61it/s]\u001b[A\n",
      "batch:  50%|████▉     | 11900/24032 [35:45<36:05,  5.60it/s]\u001b[A\n",
      "batch:  50%|████▉     | 12000/24032 [36:03<36:03,  5.56it/s]\u001b[A\n",
      "batch:  50%|█████     | 12100/24032 [36:21<35:34,  5.59it/s]\u001b[A\n",
      "batch:  51%|█████     | 12200/24032 [36:38<35:01,  5.63it/s]\u001b[A\n",
      "batch:  51%|█████     | 12300/24032 [36:56<34:52,  5.61it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12400/24032 [37:14<34:30,  5.62it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12500/24032 [37:32<34:21,  5.59it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12600/24032 [37:50<34:05,  5.59it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 12700/24032 [38:08<33:54,  5.57it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 12800/24032 [38:26<33:50,  5.53it/s]\u001b[A\n",
      "batch:  54%|█████▎    | 12900/24032 [38:44<33:35,  5.52it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 13000/24032 [39:02<33:10,  5.54it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 13100/24032 [39:20<32:40,  5.58it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 13200/24032 [39:38<32:15,  5.60it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 13300/24032 [39:56<32:03,  5.58it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 13400/24032 [40:15<32:16,  5.49it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 13500/24032 [40:35<33:10,  5.29it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13600/24032 [40:58<34:45,  5.00it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13700/24032 [41:16<33:34,  5.13it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13800/24032 [41:34<32:19,  5.28it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 13900/24032 [41:51<31:13,  5.41it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 14000/24032 [42:09<30:51,  5.42it/s]\u001b[A\n",
      "batch:  59%|█████▊    | 14100/24032 [42:27<29:58,  5.52it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 14200/24032 [42:44<29:30,  5.55it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 14300/24032 [43:05<30:30,  5.32it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 14400/24032 [43:24<30:21,  5.29it/s]\u001b[A\n",
      "batch:  60%|██████    | 14500/24032 [43:43<30:01,  5.29it/s]\u001b[A\n",
      "batch:  61%|██████    | 14600/24032 [44:02<29:28,  5.33it/s]\u001b[A\n",
      "batch:  61%|██████    | 14700/24032 [44:21<29:17,  5.31it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 14800/24032 [44:39<28:59,  5.31it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 14900/24032 [44:58<28:21,  5.37it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 15000/24032 [45:17<28:18,  5.32it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 15100/24032 [45:36<28:07,  5.29it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 15200/24032 [45:54<27:32,  5.35it/s]\u001b[A\n",
      "batch:  64%|██████▎   | 15300/24032 [46:14<27:32,  5.28it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 15400/24032 [46:33<27:18,  5.27it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 15500/24032 [46:51<26:50,  5.30it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 15600/24032 [47:11<26:38,  5.27it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 15700/24032 [47:30<26:34,  5.22it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 15800/24032 [47:48<25:57,  5.29it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 15900/24032 [48:07<25:19,  5.35it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16000/24032 [48:25<25:02,  5.35it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16100/24032 [48:44<24:47,  5.33it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16200/24032 [49:02<24:14,  5.39it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 16300/24032 [49:22<24:10,  5.33it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 16400/24032 [49:40<23:44,  5.36it/s]\u001b[A\n",
      "batch:  69%|██████▊   | 16500/24032 [49:57<22:49,  5.50it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 16600/24032 [50:14<21:59,  5.63it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 16700/24032 [50:31<21:36,  5.65it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 16800/24032 [50:49<21:20,  5.65it/s]\u001b[A\n",
      "batch:  70%|███████   | 16900/24032 [51:07<21:03,  5.64it/s]\u001b[A\n",
      "batch:  71%|███████   | 17000/24032 [51:24<20:42,  5.66it/s]\u001b[A\n",
      "batch:  71%|███████   | 17100/24032 [51:42<20:22,  5.67it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17200/24032 [52:00<20:04,  5.67it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17300/24032 [52:17<19:46,  5.67it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17400/24032 [52:34<19:21,  5.71it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 17500/24032 [52:52<18:59,  5.73it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 17600/24032 [53:09<18:37,  5.76it/s]\u001b[A\n",
      "batch:  74%|███████▎  | 17700/24032 [53:26<18:19,  5.76it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 17800/24032 [53:43<17:56,  5.79it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 17900/24032 [54:01<17:39,  5.79it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 18000/24032 [54:18<17:23,  5.78it/s]\u001b[A\n",
      "batch:  75%|███████▌  | 18100/24032 [54:35<17:02,  5.80it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 18200/24032 [54:52<16:47,  5.79it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 18300/24032 [55:10<16:31,  5.78it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18400/24032 [55:27<16:17,  5.76it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18500/24032 [55:45<16:02,  5.75it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18600/24032 [56:03<15:50,  5.71it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 18700/24032 [56:20<15:35,  5.70it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 18800/24032 [56:38<15:22,  5.67it/s]\u001b[A\n",
      "batch:  79%|███████▊  | 18900/24032 [56:55<15:02,  5.69it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 19000/24032 [57:13<14:39,  5.72it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 19100/24032 [57:31<14:29,  5.67it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 19200/24032 [57:48<14:12,  5.67it/s]\u001b[A\n",
      "batch:  80%|████████  | 19300/24032 [58:06<13:51,  5.69it/s]\u001b[A\n",
      "batch:  81%|████████  | 19400/24032 [58:23<13:27,  5.74it/s]\u001b[A\n",
      "batch:  81%|████████  | 19500/24032 [58:40<13:03,  5.79it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19600/24032 [58:57<12:44,  5.80it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19700/24032 [59:14<12:23,  5.82it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19800/24032 [59:31<12:07,  5.82it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 19900/24032 [59:48<11:46,  5.85it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 20000/24032 [1:00:05<11:31,  5.83it/s]\u001b[A\n",
      "batch:  84%|████████▎ | 20100/24032 [1:00:22<11:13,  5.84it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 20200/24032 [1:00:40<10:58,  5.82it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 20300/24032 [1:00:57<10:44,  5.79it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 20400/24032 [1:01:14<10:24,  5.82it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 20500/24032 [1:01:32<10:08,  5.80it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 20600/24032 [1:01:49<09:51,  5.81it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 20700/24032 [1:02:06<09:31,  5.83it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 20800/24032 [1:02:23<09:14,  5.83it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 20900/24032 [1:02:40<08:58,  5.82it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 21000/24032 [1:02:57<08:39,  5.84it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 21100/24032 [1:03:14<08:23,  5.82it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 21200/24032 [1:03:32<08:10,  5.77it/s]\u001b[A\n",
      "batch:  89%|████████▊ | 21300/24032 [1:03:49<07:50,  5.81it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 21400/24032 [1:04:06<07:33,  5.80it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 21500/24032 [1:04:24<07:20,  5.74it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 21600/24032 [1:04:42<07:06,  5.70it/s]\u001b[A\n",
      "batch:  90%|█████████ | 21700/24032 [1:05:00<06:49,  5.69it/s]\u001b[A\n",
      "batch:  91%|█████████ | 21800/24032 [1:05:18<06:34,  5.66it/s]\u001b[A\n",
      "batch:  91%|█████████ | 21900/24032 [1:05:35<06:18,  5.64it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22000/24032 [1:05:54<06:02,  5.61it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22100/24032 [1:06:10<05:39,  5.70it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22200/24032 [1:06:28<05:19,  5.73it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 22300/24032 [1:06:45<05:00,  5.76it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 22400/24032 [1:07:02<04:44,  5.73it/s]\u001b[A\n",
      "batch:  94%|█████████▎| 22500/24032 [1:07:20<04:28,  5.71it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 22600/24032 [1:07:38<04:11,  5.70it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 22700/24032 [1:07:55<03:51,  5.75it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 22800/24032 [1:08:12<03:35,  5.71it/s]\u001b[A\n",
      "batch:  95%|█████████▌| 22900/24032 [1:08:31<03:20,  5.64it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 23000/24032 [1:08:48<03:01,  5.67it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 23100/24032 [1:09:06<02:44,  5.65it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 23200/24032 [1:09:24<02:27,  5.66it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 23300/24032 [1:09:41<02:09,  5.67it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 23400/24032 [1:09:58<01:50,  5.72it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 23500/24032 [1:10:16<01:32,  5.74it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 23600/24032 [1:10:33<01:15,  5.74it/s]\u001b[A\n",
      "batch:  99%|█████████▊| 23700/24032 [1:10:50<00:57,  5.77it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 23800/24032 [1:11:07<00:40,  5.78it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 23900/24032 [1:11:25<00:22,  5.77it/s]\u001b[A\n",
      "batch: 100%|█████████▉| 24000/24032 [1:11:47<00:05,  5.57it/s]\u001b[A\n",
      "epoch:  40%|████      | 2/5 [2:34:06<3:54:09, 4683.20s/it]\n",
      "batch:   0%|          | 0/24032 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 0.0055          val_loss 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "batch:   0%|          | 100/24032 [00:17<1:10:07,  5.69it/s]\u001b[A\n",
      "batch:   1%|          | 200/24032 [00:34<1:09:18,  5.73it/s]\u001b[A\n",
      "batch:   1%|          | 300/24032 [00:51<1:08:44,  5.75it/s]\u001b[A\n",
      "batch:   2%|▏         | 400/24032 [01:08<1:08:01,  5.79it/s]\u001b[A\n",
      "batch:   2%|▏         | 500/24032 [01:26<1:07:37,  5.80it/s]\u001b[A\n",
      "batch:   2%|▏         | 600/24032 [01:43<1:06:58,  5.83it/s]\u001b[A\n",
      "batch:   3%|▎         | 700/24032 [02:00<1:06:48,  5.82it/s]\u001b[A\n",
      "batch:   3%|▎         | 800/24032 [02:17<1:06:23,  5.83it/s]\u001b[A\n",
      "batch:   4%|▎         | 900/24032 [02:34<1:06:11,  5.82it/s]\u001b[A\n",
      "batch:   4%|▍         | 1000/24032 [02:51<1:05:50,  5.83it/s]\u001b[A\n",
      "batch:   5%|▍         | 1100/24032 [03:09<1:06:08,  5.78it/s]\u001b[A\n",
      "batch:   5%|▍         | 1200/24032 [03:26<1:05:46,  5.79it/s]\u001b[A\n",
      "batch:   5%|▌         | 1300/24032 [03:43<1:05:16,  5.80it/s]\u001b[A\n",
      "batch:   6%|▌         | 1400/24032 [04:00<1:04:38,  5.84it/s]\u001b[A\n",
      "batch:   6%|▌         | 1500/24032 [04:17<1:04:09,  5.85it/s]\u001b[A\n",
      "batch:   7%|▋         | 1600/24032 [04:34<1:04:01,  5.84it/s]\u001b[A\n",
      "batch:   7%|▋         | 1700/24032 [04:51<1:03:27,  5.87it/s]\u001b[A\n",
      "batch:   7%|▋         | 1800/24032 [05:09<1:03:28,  5.84it/s]\u001b[A\n",
      "batch:   8%|▊         | 1900/24032 [05:26<1:03:04,  5.85it/s]\u001b[A\n",
      "batch:   8%|▊         | 2000/24032 [05:42<1:02:29,  5.88it/s]\u001b[A\n",
      "batch:   9%|▊         | 2100/24032 [05:59<1:02:13,  5.87it/s]\u001b[A\n",
      "batch:   9%|▉         | 2200/24032 [06:17<1:02:16,  5.84it/s]\u001b[A\n",
      "batch:  10%|▉         | 2300/24032 [06:34<1:01:54,  5.85it/s]\u001b[A\n",
      "batch:  10%|▉         | 2400/24032 [06:51<1:01:35,  5.85it/s]\u001b[A\n",
      "batch:  10%|█         | 2500/24032 [07:08<1:01:54,  5.80it/s]\u001b[A\n",
      "batch:  11%|█         | 2600/24032 [07:26<1:01:53,  5.77it/s]\u001b[A\n",
      "batch:  11%|█         | 2700/24032 [07:44<1:01:50,  5.75it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2800/24032 [08:01<1:01:39,  5.74it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2900/24032 [08:19<1:01:41,  5.71it/s]\u001b[A\n",
      "batch:  12%|█▏        | 3000/24032 [08:36<1:01:28,  5.70it/s]\u001b[A\n",
      "batch:  13%|█▎        | 3100/24032 [08:54<1:01:21,  5.69it/s]\u001b[A\n",
      "batch:  13%|█▎        | 3200/24032 [09:11<1:00:32,  5.73it/s]\u001b[A\n",
      "batch:  14%|█▎        | 3300/24032 [09:28<1:00:00,  5.76it/s]\u001b[A\n",
      "batch:  14%|█▍        | 3400/24032 [09:46<59:35,  5.77it/s]  \u001b[A\n",
      "batch:  15%|█▍        | 3500/24032 [10:03<59:20,  5.77it/s]\u001b[A\n",
      "batch:  15%|█▍        | 3600/24032 [10:20<58:32,  5.82it/s]\u001b[A\n",
      "batch:  15%|█▌        | 3700/24032 [10:37<58:20,  5.81it/s]\u001b[A\n",
      "batch:  16%|█▌        | 3800/24032 [10:54<57:55,  5.82it/s]\u001b[A\n",
      "batch:  16%|█▌        | 3900/24032 [11:12<58:34,  5.73it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4000/24032 [11:30<58:14,  5.73it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4100/24032 [11:47<57:54,  5.74it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4200/24032 [12:05<57:47,  5.72it/s]\u001b[A\n",
      "batch:  18%|█▊        | 4300/24032 [12:22<57:37,  5.71it/s]\u001b[A\n",
      "batch:  18%|█▊        | 4400/24032 [12:40<57:20,  5.71it/s]\u001b[A\n",
      "batch:  19%|█▊        | 4500/24032 [12:57<56:47,  5.73it/s]\u001b[A\n",
      "batch:  19%|█▉        | 4600/24032 [13:14<55:57,  5.79it/s]\u001b[A\n",
      "batch:  20%|█▉        | 4700/24032 [13:31<55:49,  5.77it/s]\u001b[A\n",
      "batch:  20%|█▉        | 4800/24032 [13:49<55:25,  5.78it/s]\u001b[A\n",
      "batch:  20%|██        | 4900/24032 [14:06<54:51,  5.81it/s]\u001b[A\n",
      "batch:  21%|██        | 5000/24032 [14:23<54:21,  5.83it/s]\u001b[A\n",
      "batch:  21%|██        | 5100/24032 [14:40<53:55,  5.85it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5200/24032 [14:57<53:50,  5.83it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5300/24032 [15:14<53:35,  5.83it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5400/24032 [15:31<53:16,  5.83it/s]\u001b[A\n",
      "batch:  23%|██▎       | 5500/24032 [15:49<53:15,  5.80it/s]\u001b[A\n",
      "batch:  23%|██▎       | 5600/24032 [16:06<53:10,  5.78it/s]\u001b[A\n",
      "batch:  24%|██▎       | 5700/24032 [16:24<53:02,  5.76it/s]\u001b[A\n",
      "batch:  24%|██▍       | 5800/24032 [16:42<53:19,  5.70it/s]\u001b[A\n",
      "batch:  25%|██▍       | 5900/24032 [17:01<54:15,  5.57it/s]\u001b[A\n",
      "batch:  25%|██▍       | 6000/24032 [17:19<54:31,  5.51it/s]\u001b[A\n",
      "batch:  25%|██▌       | 6100/24032 [17:38<54:54,  5.44it/s]\u001b[A\n",
      "batch:  26%|██▌       | 6200/24032 [17:56<53:53,  5.51it/s]\u001b[A\n",
      "batch:  26%|██▌       | 6300/24032 [18:13<52:34,  5.62it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6400/24032 [18:30<51:45,  5.68it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6500/24032 [18:47<51:04,  5.72it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6600/24032 [19:04<50:41,  5.73it/s]\u001b[A\n",
      "batch:  28%|██▊       | 6700/24032 [19:22<50:21,  5.74it/s]\u001b[A\n",
      "batch:  28%|██▊       | 6800/24032 [19:39<49:40,  5.78it/s]\u001b[A\n",
      "batch:  29%|██▊       | 6900/24032 [19:56<49:09,  5.81it/s]\u001b[A\n",
      "batch:  29%|██▉       | 7000/24032 [20:13<48:50,  5.81it/s]\u001b[A\n",
      "batch:  30%|██▉       | 7100/24032 [20:30<48:35,  5.81it/s]\u001b[A\n",
      "batch:  30%|██▉       | 7200/24032 [20:47<48:14,  5.82it/s]\u001b[A\n",
      "batch:  30%|███       | 7300/24032 [21:04<47:55,  5.82it/s]\u001b[A\n",
      "batch:  31%|███       | 7400/24032 [21:21<47:23,  5.85it/s]\u001b[A\n",
      "batch:  31%|███       | 7500/24032 [21:38<47:08,  5.85it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7600/24032 [21:56<47:02,  5.82it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7700/24032 [22:13<47:04,  5.78it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7800/24032 [22:31<46:48,  5.78it/s]\u001b[A\n",
      "batch:  33%|███▎      | 7900/24032 [22:48<46:25,  5.79it/s]\u001b[A\n",
      "batch:  33%|███▎      | 8000/24032 [23:05<45:46,  5.84it/s]\u001b[A\n",
      "batch:  34%|███▎      | 8100/24032 [23:22<45:27,  5.84it/s]\u001b[A\n",
      "batch:  34%|███▍      | 8200/24032 [23:39<45:12,  5.84it/s]\u001b[A\n",
      "batch:  35%|███▍      | 8300/24032 [23:56<44:38,  5.87it/s]\u001b[A\n",
      "batch:  35%|███▍      | 8400/24032 [24:13<44:19,  5.88it/s]\u001b[A\n",
      "batch:  35%|███▌      | 8500/24032 [24:31<44:59,  5.75it/s]\u001b[A\n",
      "batch:  36%|███▌      | 8600/24032 [24:49<44:57,  5.72it/s]\u001b[A\n",
      "batch:  36%|███▌      | 8700/24032 [25:06<44:36,  5.73it/s]\u001b[A\n",
      "batch:  37%|███▋      | 8800/24032 [25:24<44:25,  5.71it/s]\u001b[A\n",
      "batch:  37%|███▋      | 8900/24032 [25:41<43:56,  5.74it/s]\u001b[A\n",
      "batch:  37%|███▋      | 9000/24032 [25:58<43:40,  5.74it/s]\u001b[A\n",
      "batch:  38%|███▊      | 9100/24032 [26:16<43:19,  5.74it/s]\u001b[A\n",
      "batch:  38%|███▊      | 9200/24032 [26:33<42:54,  5.76it/s]\u001b[A\n",
      "batch:  39%|███▊      | 9300/24032 [26:50<42:13,  5.81it/s]\u001b[A\n",
      "batch:  39%|███▉      | 9400/24032 [27:07<41:51,  5.83it/s]\u001b[A\n",
      "batch:  40%|███▉      | 9500/24032 [27:24<41:35,  5.82it/s]\u001b[A\n",
      "batch:  40%|███▉      | 9600/24032 [27:41<41:19,  5.82it/s]\u001b[A\n",
      "batch:  40%|████      | 9700/24032 [27:58<40:58,  5.83it/s]\u001b[A\n",
      "batch:  41%|████      | 9800/24032 [28:15<40:41,  5.83it/s]\u001b[A\n",
      "batch:  41%|████      | 9900/24032 [28:33<40:42,  5.79it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10000/24032 [28:51<40:35,  5.76it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10100/24032 [29:08<40:32,  5.73it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10200/24032 [29:26<40:43,  5.66it/s]\u001b[A\n",
      "batch:  43%|████▎     | 10300/24032 [29:44<40:32,  5.65it/s]\u001b[A\n",
      "batch:  43%|████▎     | 10400/24032 [30:01<39:50,  5.70it/s]\u001b[A\n",
      "batch:  44%|████▎     | 10500/24032 [30:19<39:49,  5.66it/s]\u001b[A\n",
      "batch:  44%|████▍     | 10600/24032 [30:37<39:33,  5.66it/s]\u001b[A\n",
      "batch:  45%|████▍     | 10700/24032 [30:54<38:45,  5.73it/s]\u001b[A\n",
      "batch:  45%|████▍     | 10800/24032 [31:11<38:20,  5.75it/s]\u001b[A\n",
      "batch:  45%|████▌     | 10900/24032 [31:28<37:57,  5.77it/s]\u001b[A\n",
      "batch:  46%|████▌     | 11000/24032 [31:46<37:33,  5.78it/s]\u001b[A\n",
      "batch:  46%|████▌     | 11100/24032 [32:03<37:11,  5.79it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11200/24032 [32:20<36:41,  5.83it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11300/24032 [32:38<37:02,  5.73it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11400/24032 [32:56<36:54,  5.71it/s]\u001b[A\n",
      "batch:  48%|████▊     | 11500/24032 [33:13<36:44,  5.69it/s]\u001b[A\n",
      "batch:  48%|████▊     | 11600/24032 [33:31<36:41,  5.65it/s]\u001b[A\n",
      "batch:  49%|████▊     | 11700/24032 [33:49<36:30,  5.63it/s]\u001b[A\n",
      "batch:  49%|████▉     | 11800/24032 [34:07<36:23,  5.60it/s]\u001b[A\n",
      "batch:  50%|████▉     | 11900/24032 [34:25<35:56,  5.62it/s]\u001b[A\n",
      "batch:  50%|████▉     | 12000/24032 [34:43<35:39,  5.62it/s]\u001b[A\n",
      "batch:  50%|█████     | 12100/24032 [35:00<34:56,  5.69it/s]\u001b[A\n",
      "batch:  51%|█████     | 12200/24032 [35:17<34:29,  5.72it/s]\u001b[A\n",
      "batch:  51%|█████     | 12300/24032 [35:34<34:09,  5.73it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12400/24032 [35:52<33:46,  5.74it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12500/24032 [36:09<33:27,  5.74it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12600/24032 [36:27<33:14,  5.73it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 12700/24032 [36:44<33:04,  5.71it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 12800/24032 [37:02<32:54,  5.69it/s]\u001b[A\n",
      "batch:  54%|█████▎    | 12900/24032 [37:20<32:35,  5.69it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 13000/24032 [37:37<32:07,  5.72it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 13100/24032 [37:54<31:34,  5.77it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 13200/24032 [38:11<31:18,  5.77it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 13300/24032 [38:28<30:52,  5.79it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 13400/24032 [38:45<30:18,  5.85it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 13500/24032 [39:02<29:57,  5.86it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13600/24032 [39:19<29:45,  5.84it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13700/24032 [39:36<29:25,  5.85it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13800/24032 [39:53<29:03,  5.87it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 13900/24032 [40:11<28:55,  5.84it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 14000/24032 [40:28<28:43,  5.82it/s]\u001b[A\n",
      "batch:  59%|█████▊    | 14100/24032 [40:45<28:26,  5.82it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 14200/24032 [41:03<28:30,  5.75it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 14300/24032 [41:22<28:51,  5.62it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 14400/24032 [41:40<28:54,  5.55it/s]\u001b[A\n",
      "batch:  60%|██████    | 14500/24032 [42:01<29:53,  5.31it/s]\u001b[A\n",
      "batch:  61%|██████    | 14600/24032 [42:22<30:35,  5.14it/s]\u001b[A\n",
      "batch:  61%|██████    | 14700/24032 [42:40<29:31,  5.27it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 14800/24032 [42:58<28:45,  5.35it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 14900/24032 [43:15<28:00,  5.43it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 15000/24032 [43:33<27:30,  5.47it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 15100/24032 [43:52<27:07,  5.49it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 15200/24032 [44:09<26:40,  5.52it/s]\u001b[A\n",
      "batch:  64%|██████▎   | 15300/24032 [44:27<26:02,  5.59it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 15400/24032 [44:44<25:32,  5.63it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 15500/24032 [45:01<24:59,  5.69it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 15600/24032 [45:19<24:33,  5.72it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 15700/24032 [45:36<24:13,  5.73it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 15800/24032 [45:53<23:48,  5.76it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 15900/24032 [46:10<23:12,  5.84it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16000/24032 [46:27<22:50,  5.86it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16100/24032 [46:44<22:35,  5.85it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16200/24032 [47:01<22:18,  5.85it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 16300/24032 [47:18<21:51,  5.90it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 16400/24032 [47:35<21:35,  5.89it/s]\u001b[A\n",
      "batch:  69%|██████▊   | 16500/24032 [47:52<21:22,  5.88it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 16600/24032 [48:09<21:07,  5.86it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 16700/24032 [48:26<20:50,  5.86it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 16800/24032 [48:43<20:32,  5.87it/s]\u001b[A\n",
      "batch:  70%|███████   | 16900/24032 [49:00<20:18,  5.85it/s]\u001b[A\n",
      "batch:  71%|███████   | 17000/24032 [49:17<19:58,  5.87it/s]\u001b[A\n",
      "batch:  71%|███████   | 17100/24032 [49:34<19:38,  5.88it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17200/24032 [49:51<19:22,  5.88it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17300/24032 [50:08<19:03,  5.89it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17400/24032 [50:25<18:45,  5.89it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 17500/24032 [50:42<18:25,  5.91it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 17600/24032 [50:58<18:06,  5.92it/s]\u001b[A\n",
      "batch:  74%|███████▎  | 17700/24032 [51:16<17:54,  5.90it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 17800/24032 [51:32<17:35,  5.90it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 17900/24032 [51:49<17:18,  5.90it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 18000/24032 [52:06<17:04,  5.89it/s]\u001b[A\n",
      "batch:  75%|███████▌  | 18100/24032 [52:24<16:51,  5.87it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 18200/24032 [52:41<16:35,  5.86it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 18300/24032 [52:58<16:15,  5.88it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18400/24032 [53:15<15:54,  5.90it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18500/24032 [53:32<15:42,  5.87it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18600/24032 [53:49<15:29,  5.84it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 18700/24032 [54:06<15:09,  5.86it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 18800/24032 [54:23<14:53,  5.86it/s]\u001b[A\n",
      "batch:  79%|███████▊  | 18900/24032 [54:40<14:34,  5.87it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 19000/24032 [54:57<14:21,  5.84it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 19100/24032 [55:14<14:00,  5.87it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 19200/24032 [55:31<13:43,  5.87it/s]\u001b[A\n",
      "batch:  80%|████████  | 19300/24032 [55:48<13:26,  5.87it/s]\u001b[A\n",
      "batch:  81%|████████  | 19400/24032 [56:05<13:09,  5.86it/s]\u001b[A\n",
      "batch:  81%|████████  | 19500/24032 [56:23<12:55,  5.84it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19600/24032 [56:39<12:32,  5.89it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19700/24032 [56:56<12:15,  5.89it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19800/24032 [57:13<11:57,  5.89it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 19900/24032 [57:30<11:43,  5.88it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 20000/24032 [57:48<11:28,  5.86it/s]\u001b[A\n",
      "batch:  84%|████████▎ | 20100/24032 [58:05<11:11,  5.85it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 20200/24032 [58:22<10:53,  5.86it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 20300/24032 [58:39<10:36,  5.87it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 20400/24032 [58:56<10:17,  5.88it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 20500/24032 [59:13<10:04,  5.84it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 20600/24032 [59:30<09:46,  5.85it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 20700/24032 [59:47<09:29,  5.86it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 20800/24032 [1:00:04<09:12,  5.85it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 20900/24032 [1:00:21<08:51,  5.89it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 21000/24032 [1:00:38<08:36,  5.87it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 21100/24032 [1:00:55<08:21,  5.85it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 21200/24032 [1:01:12<08:01,  5.88it/s]\u001b[A\n",
      "batch:  89%|████████▊ | 21300/24032 [1:01:29<07:45,  5.86it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 21400/24032 [1:01:46<07:30,  5.85it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 21500/24032 [1:02:04<07:12,  5.85it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 21600/24032 [1:02:21<06:55,  5.86it/s]\u001b[A\n",
      "batch:  90%|█████████ | 21700/24032 [1:02:38<06:38,  5.85it/s]\u001b[A\n",
      "batch:  91%|█████████ | 21800/24032 [1:02:55<06:21,  5.85it/s]\u001b[A\n",
      "batch:  91%|█████████ | 21900/24032 [1:03:12<06:03,  5.86it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22000/24032 [1:03:29<05:45,  5.87it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22100/24032 [1:03:46<05:30,  5.85it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22200/24032 [1:04:03<05:12,  5.87it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 22300/24032 [1:04:20<04:53,  5.90it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 22400/24032 [1:04:37<04:36,  5.90it/s]\u001b[A\n",
      "batch:  94%|█████████▎| 22500/24032 [1:04:53<04:19,  5.91it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 23000/24032 [1:06:19<02:55,  5.88it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 23100/24032 [1:06:36<02:38,  5.86it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 23200/24032 [1:06:53<02:21,  5.86it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 23300/24032 [1:07:10<02:05,  5.84it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 23400/24032 [1:07:27<01:48,  5.85it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 23500/24032 [1:07:44<01:30,  5.85it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 23600/24032 [1:08:01<01:13,  5.84it/s]\u001b[A\n",
      "batch:  99%|█████████▊| 23700/24032 [1:08:19<00:57,  5.81it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 23800/24032 [1:08:36<00:39,  5.84it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 23900/24032 [1:08:53<00:22,  5.84it/s]\u001b[A\n",
      "batch: 100%|█████████▉| 24000/24032 [1:09:15<00:05,  5.78it/s]\u001b[A\n",
      "epoch:  60%|██████    | 3/5 [3:46:02<2:32:26, 4573.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 003 train_loss: 0.0052          val_loss 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "batch:   0%|          | 0/24032 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   0%|          | 100/24032 [00:17<1:10:23,  5.67it/s]\u001b[A\n",
      "batch:   1%|          | 200/24032 [00:34<1:09:15,  5.73it/s]\u001b[A\n",
      "batch:   1%|          | 300/24032 [00:52<1:09:22,  5.70it/s]\u001b[A\n",
      "batch:   2%|▏         | 400/24032 [01:09<1:08:05,  5.78it/s]\u001b[A\n",
      "batch:   2%|▏         | 500/24032 [01:26<1:07:24,  5.82it/s]\u001b[A\n",
      "batch:   2%|▏         | 600/24032 [01:42<1:06:33,  5.87it/s]\u001b[A\n",
      "batch:   3%|▎         | 700/24032 [01:59<1:06:27,  5.85it/s]\u001b[A\n",
      "batch:   3%|▎         | 800/24032 [02:17<1:06:13,  5.85it/s]\u001b[A\n",
      "batch:   4%|▎         | 900/24032 [02:33<1:05:27,  5.89it/s]\u001b[A\n",
      "batch:   4%|▍         | 1000/24032 [02:50<1:05:11,  5.89it/s]\u001b[A\n",
      "batch:   5%|▍         | 1100/24032 [03:07<1:05:11,  5.86it/s]\u001b[A\n",
      "batch:   5%|▍         | 1200/24032 [03:25<1:05:06,  5.85it/s]\u001b[A\n",
      "batch:   5%|▌         | 1300/24032 [03:42<1:04:36,  5.86it/s]\u001b[A\n",
      "batch:   6%|▌         | 1400/24032 [03:59<1:04:08,  5.88it/s]\u001b[A\n",
      "batch:   6%|▌         | 1500/24032 [04:16<1:04:01,  5.87it/s]\u001b[A\n",
      "batch:   7%|▋         | 1600/24032 [04:33<1:03:59,  5.84it/s]\u001b[A\n",
      "batch:   7%|▋         | 1700/24032 [04:50<1:04:04,  5.81it/s]\u001b[A\n",
      "batch:   7%|▋         | 1800/24032 [05:08<1:03:42,  5.82it/s]\u001b[A\n",
      "batch:   8%|▊         | 1900/24032 [05:25<1:03:28,  5.81it/s]\u001b[A\n",
      "batch:   8%|▊         | 2000/24032 [05:42<1:03:14,  5.81it/s]\u001b[A\n",
      "batch:   9%|▊         | 2100/24032 [05:59<1:03:13,  5.78it/s]\u001b[A\n",
      "batch:   9%|▉         | 2200/24032 [06:17<1:02:50,  5.79it/s]\u001b[A\n",
      "batch:  10%|▉         | 2300/24032 [06:34<1:02:35,  5.79it/s]\u001b[A\n",
      "batch:  10%|▉         | 2400/24032 [06:51<1:02:00,  5.81it/s]\u001b[A\n",
      "batch:  10%|█         | 2500/24032 [07:08<1:01:19,  5.85it/s]\u001b[A\n",
      "batch:  11%|█         | 2600/24032 [07:25<1:01:00,  5.86it/s]\u001b[A\n",
      "batch:  11%|█         | 2700/24032 [07:42<1:00:22,  5.89it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2800/24032 [07:59<1:00:14,  5.87it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2900/24032 [08:16<1:00:12,  5.85it/s]\u001b[A\n",
      "batch:  12%|█▏        | 3000/24032 [08:33<59:39,  5.87it/s]  \u001b[A\n",
      "batch:  13%|█▎        | 3100/24032 [08:50<59:13,  5.89it/s]\u001b[A\n",
      "batch:  13%|█▎        | 3200/24032 [09:07<59:20,  5.85it/s]\u001b[A\n",
      "batch:  14%|█▎        | 3300/24032 [09:24<59:18,  5.83it/s]\u001b[A\n",
      "batch:  14%|█▍        | 3400/24032 [09:41<58:47,  5.85it/s]\u001b[A\n",
      "batch:  15%|█▍        | 3500/24032 [09:58<58:16,  5.87it/s]\u001b[A\n",
      "batch:  15%|█▍        | 3600/24032 [10:16<58:15,  5.85it/s]\u001b[A\n",
      "batch:  15%|█▌        | 3700/24032 [10:32<57:46,  5.87it/s]\u001b[A\n",
      "batch:  16%|█▌        | 3800/24032 [10:50<58:00,  5.81it/s]\u001b[A\n",
      "batch:  16%|█▌        | 3900/24032 [11:07<57:52,  5.80it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4000/24032 [11:25<57:35,  5.80it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4100/24032 [11:42<57:15,  5.80it/s]\u001b[A\n",
      "batch:  17%|█▋        | 4200/24032 [11:59<57:12,  5.78it/s]\u001b[A\n",
      "batch:  18%|█▊        | 4300/24032 [12:17<57:03,  5.76it/s]\u001b[A\n",
      "batch:  18%|█▊        | 4400/24032 [12:34<56:43,  5.77it/s]\u001b[A\n",
      "batch:  19%|█▊        | 4500/24032 [12:51<56:27,  5.77it/s]\u001b[A\n",
      "batch:  19%|█▉        | 4600/24032 [13:09<56:00,  5.78it/s]\u001b[A\n",
      "batch:  20%|█▉        | 4700/24032 [13:25<55:18,  5.82it/s]\u001b[A\n",
      "batch:  20%|█▉        | 4800/24032 [13:43<54:57,  5.83it/s]\u001b[A\n",
      "batch:  20%|██        | 4900/24032 [14:00<54:44,  5.83it/s]\u001b[A\n",
      "batch:  21%|██        | 5000/24032 [14:17<54:11,  5.85it/s]\u001b[A\n",
      "batch:  21%|██        | 5100/24032 [14:34<53:49,  5.86it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5200/24032 [14:51<53:50,  5.83it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5300/24032 [15:08<53:36,  5.82it/s]\u001b[A\n",
      "batch:  22%|██▏       | 5400/24032 [15:25<53:19,  5.82it/s]\u001b[A\n",
      "batch:  23%|██▎       | 5500/24032 [15:43<53:26,  5.78it/s]\u001b[A\n",
      "batch:  23%|██▎       | 5600/24032 [16:00<52:53,  5.81it/s]\u001b[A\n",
      "batch:  24%|██▎       | 5700/24032 [16:17<52:35,  5.81it/s]\u001b[A\n",
      "batch:  24%|██▍       | 5800/24032 [16:34<52:15,  5.81it/s]\u001b[A\n",
      "batch:  25%|██▍       | 5900/24032 [16:52<52:08,  5.80it/s]\u001b[A\n",
      "batch:  25%|██▍       | 6000/24032 [17:09<51:38,  5.82it/s]\u001b[A\n",
      "batch:  25%|██▌       | 6100/24032 [17:26<51:19,  5.82it/s]\u001b[A\n",
      "batch:  26%|██▌       | 6200/24032 [17:43<50:57,  5.83it/s]\u001b[A\n",
      "batch:  26%|██▌       | 6300/24032 [18:00<50:33,  5.85it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6400/24032 [18:18<50:38,  5.80it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6500/24032 [18:34<50:00,  5.84it/s]\u001b[A\n",
      "batch:  27%|██▋       | 6600/24032 [18:52<49:40,  5.85it/s]\u001b[A\n",
      "batch:  28%|██▊       | 6700/24032 [19:09<49:53,  5.79it/s]\u001b[A\n",
      "batch:  28%|██▊       | 6800/24032 [19:27<49:41,  5.78it/s]\u001b[A\n",
      "batch:  29%|██▊       | 6900/24032 [19:44<49:27,  5.77it/s]\u001b[A\n",
      "batch:  29%|██▉       | 7000/24032 [20:02<49:25,  5.74it/s]\u001b[A\n",
      "batch:  30%|██▉       | 7100/24032 [20:19<48:58,  5.76it/s]\u001b[A\n",
      "batch:  30%|██▉       | 7200/24032 [20:36<48:21,  5.80it/s]\u001b[A\n",
      "batch:  30%|███       | 7300/24032 [20:53<48:04,  5.80it/s]\u001b[A\n",
      "batch:  31%|███       | 7400/24032 [21:10<47:39,  5.82it/s]\u001b[A\n",
      "batch:  31%|███       | 7500/24032 [21:27<47:00,  5.86it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7600/24032 [21:44<46:56,  5.83it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7700/24032 [22:01<46:48,  5.81it/s]\u001b[A\n",
      "batch:  32%|███▏      | 7800/24032 [22:19<46:24,  5.83it/s]\u001b[A\n",
      "batch:  33%|███▎      | 7900/24032 [22:35<45:53,  5.86it/s]\u001b[A\n",
      "batch:  33%|███▎      | 8000/24032 [22:53<45:44,  5.84it/s]\u001b[A\n",
      "batch:  34%|███▎      | 8100/24032 [23:10<45:15,  5.87it/s]\u001b[A\n",
      "batch:  34%|███▍      | 8200/24032 [23:26<44:48,  5.89it/s]\u001b[A\n",
      "batch:  35%|███▍      | 8300/24032 [23:43<44:19,  5.92it/s]\u001b[A\n",
      "batch:  38%|███▊      | 9100/24032 [26:02<42:59,  5.79it/s]\u001b[A\n",
      "batch:  38%|███▊      | 9200/24032 [26:19<42:50,  5.77it/s]\u001b[A\n",
      "batch:  39%|███▊      | 9300/24032 [26:36<42:22,  5.79it/s]\u001b[A\n",
      "batch:  39%|███▉      | 9400/24032 [26:53<42:02,  5.80it/s]\u001b[A\n",
      "batch:  40%|███▉      | 9500/24032 [27:11<41:43,  5.80it/s]\u001b[A\n",
      "batch:  40%|███▉      | 9600/24032 [27:28<41:17,  5.83it/s]\u001b[A\n",
      "batch:  40%|████      | 9700/24032 [27:44<40:36,  5.88it/s]\u001b[A\n",
      "batch:  41%|████      | 9800/24032 [28:01<40:19,  5.88it/s]\u001b[A\n",
      "batch:  41%|████      | 9900/24032 [28:19<40:12,  5.86it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10000/24032 [28:36<40:01,  5.84it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10100/24032 [28:53<39:45,  5.84it/s]\u001b[A\n",
      "batch:  42%|████▏     | 10200/24032 [29:10<39:35,  5.82it/s]\u001b[A\n",
      "batch:  43%|████▎     | 10300/24032 [29:27<39:13,  5.84it/s]\u001b[A\n",
      "batch:  43%|████▎     | 10400/24032 [29:44<38:56,  5.83it/s]\u001b[A\n",
      "batch:  44%|████▎     | 10500/24032 [30:02<38:45,  5.82it/s]\u001b[A\n",
      "batch:  44%|████▍     | 10600/24032 [30:19<38:33,  5.81it/s]\u001b[A\n",
      "batch:  45%|████▍     | 10700/24032 [30:36<38:07,  5.83it/s]\u001b[A\n",
      "batch:  45%|████▍     | 10800/24032 [30:53<37:38,  5.86it/s]\u001b[A\n",
      "batch:  45%|████▌     | 10900/24032 [31:10<37:19,  5.86it/s]\u001b[A\n",
      "batch:  46%|████▌     | 11000/24032 [31:27<37:07,  5.85it/s]\u001b[A\n",
      "batch:  46%|████▌     | 11100/24032 [31:44<36:36,  5.89it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11200/24032 [32:01<36:37,  5.84it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11300/24032 [32:18<36:24,  5.83it/s]\u001b[A\n",
      "batch:  47%|████▋     | 11400/24032 [32:36<36:06,  5.83it/s]\u001b[A\n",
      "batch:  48%|████▊     | 11500/24032 [32:53<35:41,  5.85it/s]\u001b[A\n",
      "batch:  48%|████▊     | 11600/24032 [33:10<35:42,  5.80it/s]\u001b[A\n",
      "batch:  49%|████▊     | 11700/24032 [33:27<35:26,  5.80it/s]\u001b[A\n",
      "batch:  49%|████▉     | 11800/24032 [33:44<34:56,  5.84it/s]\u001b[A\n",
      "batch:  50%|████▉     | 11900/24032 [34:02<34:53,  5.80it/s]\u001b[A\n",
      "batch:  50%|████▉     | 12000/24032 [34:19<34:37,  5.79it/s]\u001b[A\n",
      "batch:  50%|█████     | 12100/24032 [34:36<34:03,  5.84it/s]\u001b[A\n",
      "batch:  51%|█████     | 12200/24032 [34:53<33:47,  5.83it/s]\u001b[A\n",
      "batch:  51%|█████     | 12300/24032 [35:10<33:32,  5.83it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12400/24032 [35:28<33:18,  5.82it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12500/24032 [35:45<32:55,  5.84it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 12600/24032 [36:01<32:32,  5.85it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 12700/24032 [36:19<32:22,  5.83it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 12800/24032 [36:36<31:54,  5.87it/s]\u001b[A\n",
      "batch:  54%|█████▎    | 12900/24032 [36:53<31:35,  5.87it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 13000/24032 [37:11<31:53,  5.77it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 13100/24032 [37:28<31:32,  5.78it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 13200/24032 [37:45<31:18,  5.77it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 13300/24032 [38:02<30:51,  5.80it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 13400/24032 [38:20<30:39,  5.78it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 13500/24032 [38:37<30:21,  5.78it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13600/24032 [38:55<30:12,  5.76it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13700/24032 [39:12<29:58,  5.75it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 13800/24032 [39:30<29:52,  5.71it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 13900/24032 [39:47<29:20,  5.76it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 14000/24032 [40:04<28:55,  5.78it/s]\u001b[A\n",
      "batch:  59%|█████▊    | 14100/24032 [40:21<28:32,  5.80it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 14200/24032 [40:38<28:05,  5.83it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 14300/24032 [40:55<27:48,  5.83it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 14400/24032 [41:12<27:20,  5.87it/s]\u001b[A\n",
      "batch:  60%|██████    | 14500/24032 [41:29<26:54,  5.91it/s]\u001b[A\n",
      "batch:  61%|██████    | 14600/24032 [41:46<26:38,  5.90it/s]\u001b[A\n",
      "batch:  61%|██████    | 14700/24032 [42:03<26:18,  5.91it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 14800/24032 [42:20<26:05,  5.90it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 14900/24032 [42:37<26:00,  5.85it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 15000/24032 [42:54<25:45,  5.85it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 15100/24032 [43:12<25:39,  5.80it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 15200/24032 [43:29<25:26,  5.79it/s]\u001b[A\n",
      "batch:  64%|██████▎   | 15300/24032 [43:46<25:06,  5.80it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 15400/24032 [44:04<24:54,  5.78it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 15500/24032 [44:21<24:31,  5.80it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 15600/24032 [44:38<24:21,  5.77it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 15700/24032 [44:56<24:08,  5.75it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 15800/24032 [45:13<23:50,  5.76it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 15900/24032 [45:31<23:36,  5.74it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16000/24032 [45:48<23:13,  5.77it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16100/24032 [46:05<23:02,  5.74it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 16200/24032 [46:23<22:54,  5.70it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 16300/24032 [46:40<22:27,  5.74it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 16400/24032 [46:58<22:04,  5.76it/s]\u001b[A\n",
      "batch:  69%|██████▊   | 16500/24032 [47:15<21:38,  5.80it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 16600/24032 [47:31<21:12,  5.84it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 16700/24032 [47:49<21:03,  5.80it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 16800/24032 [48:06<20:45,  5.81it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 17400/24032 [49:51<19:22,  5.71it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 17500/24032 [50:09<19:03,  5.71it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 17600/24032 [50:26<18:38,  5.75it/s]\u001b[A\n",
      "batch:  74%|███████▎  | 17700/24032 [50:44<18:31,  5.70it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 17800/24032 [51:01<18:15,  5.69it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 17900/24032 [51:19<17:57,  5.69it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 18000/24032 [51:37<17:44,  5.67it/s]\u001b[A\n",
      "batch:  75%|███████▌  | 18100/24032 [51:54<17:26,  5.67it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 18200/24032 [52:12<17:04,  5.69it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 18300/24032 [52:30<16:50,  5.67it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18400/24032 [52:48<16:37,  5.65it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18500/24032 [53:05<16:19,  5.65it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 18600/24032 [53:22<15:48,  5.73it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 18700/24032 [53:40<15:34,  5.71it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 18800/24032 [53:57<15:18,  5.69it/s]\u001b[A\n",
      "batch:  79%|███████▊  | 18900/24032 [54:15<15:00,  5.70it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 19000/24032 [54:32<14:38,  5.73it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 19100/24032 [54:50<14:19,  5.74it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 19200/24032 [55:07<13:59,  5.76it/s]\u001b[A\n",
      "batch:  80%|████████  | 19300/24032 [55:24<13:42,  5.76it/s]\u001b[A\n",
      "batch:  81%|████████  | 19400/24032 [55:41<13:23,  5.76it/s]\u001b[A\n",
      "batch:  81%|████████  | 19500/24032 [55:59<13:03,  5.78it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19600/24032 [56:15<12:39,  5.83it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19700/24032 [56:33<12:28,  5.79it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 19800/24032 [56:50<12:08,  5.81it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 19900/24032 [57:08<11:54,  5.79it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 20000/24032 [57:25<11:39,  5.76it/s]\u001b[A\n",
      "batch:  84%|████████▎ | 20100/24032 [57:43<11:29,  5.71it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 20200/24032 [58:00<11:08,  5.73it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 20300/24032 [58:18<10:49,  5.74it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 20400/24032 [58:35<10:30,  5.76it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 20500/24032 [58:52<10:06,  5.82it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 20600/24032 [59:08<09:45,  5.86it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 20700/24032 [59:25<09:28,  5.86it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 20800/24032 [59:42<09:09,  5.88it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 20900/24032 [1:00:00<08:57,  5.82it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 21000/24032 [1:00:17<08:43,  5.79it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 21100/24032 [1:00:35<08:26,  5.79it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 21200/24032 [1:00:52<08:11,  5.76it/s]\u001b[A\n",
      "batch:  89%|████████▊ | 21300/24032 [1:01:10<07:54,  5.76it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 21400/24032 [1:01:26<07:33,  5.80it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 21500/24032 [1:01:44<07:16,  5.81it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 21600/24032 [1:02:01<07:00,  5.78it/s]\u001b[A\n",
      "batch:  90%|█████████ | 21700/24032 [1:02:19<06:45,  5.75it/s]\u001b[A\n",
      "batch:  91%|█████████ | 21800/24032 [1:02:37<06:34,  5.65it/s]\u001b[A\n",
      "batch:  91%|█████████ | 21900/24032 [1:02:55<06:16,  5.66it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22000/24032 [1:03:12<05:57,  5.69it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22100/24032 [1:03:30<05:40,  5.67it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 22200/24032 [1:03:47<05:21,  5.71it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 22300/24032 [1:04:04<05:02,  5.73it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 22400/24032 [1:04:22<04:43,  5.75it/s]\u001b[A\n",
      "batch:  94%|█████████▎| 22500/24032 [1:04:39<04:27,  5.72it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 22600/24032 [1:04:58<04:15,  5.60it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 22700/24032 [1:05:17<04:02,  5.50it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 22800/24032 [1:05:36<03:47,  5.42it/s]\u001b[A\n",
      "batch:  95%|█████████▌| 22900/24032 [1:05:56<03:32,  5.34it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 23000/24032 [1:06:15<03:15,  5.29it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 23100/24032 [1:06:34<02:57,  5.25it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 23500/24032 [1:07:51<01:43,  5.13it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 23600/24032 [1:08:10<01:23,  5.20it/s]\u001b[A\n",
      "batch:  99%|█████████▊| 23700/24032 [1:08:29<01:03,  5.21it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 23800/24032 [1:08:48<00:44,  5.22it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 23900/24032 [1:09:07<00:25,  5.25it/s]\u001b[A\n",
      "batch: 100%|█████████▉| 24000/24032 [1:09:31<00:05,  5.75it/s]\u001b[A\n",
      "epoch:  80%|████████  | 4/5 [4:58:33<1:15:06, 4506.46s/it]\n",
      "batch:   0%|          | 0/24032 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 004 train_loss: 0.0051          val_loss 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "batch:   0%|          | 100/24032 [00:19<1:15:49,  5.26it/s]\u001b[A\n",
      "batch:   1%|          | 200/24032 [00:37<1:14:36,  5.32it/s]\u001b[A\n",
      "batch:   1%|          | 300/24032 [00:56<1:15:04,  5.27it/s]\u001b[A\n",
      "batch:   2%|▏         | 400/24032 [01:15<1:14:25,  5.29it/s]\u001b[A\n",
      "batch:   2%|▏         | 500/24032 [01:35<1:15:30,  5.19it/s]\u001b[A\n",
      "batch:   2%|▏         | 600/24032 [01:54<1:14:45,  5.22it/s]\u001b[A\n",
      "batch:   3%|▎         | 700/24032 [02:13<1:14:03,  5.25it/s]\u001b[A\n",
      "batch:   3%|▎         | 800/24032 [02:31<1:13:14,  5.29it/s]\u001b[A\n",
      "batch:   4%|▎         | 900/24032 [02:50<1:12:21,  5.33it/s]\u001b[A\n",
      "batch:   4%|▍         | 1000/24032 [03:09<1:12:31,  5.29it/s]\u001b[A\n",
      "batch:   5%|▍         | 1100/24032 [03:28<1:11:55,  5.31it/s]\u001b[A\n",
      "batch:   5%|▍         | 1200/24032 [03:45<1:10:10,  5.42it/s]\u001b[A\n",
      "batch:   5%|▌         | 1300/24032 [04:03<1:08:45,  5.51it/s]\u001b[A\n",
      "batch:   6%|▌         | 1400/24032 [04:20<1:07:50,  5.56it/s]\u001b[A\n",
      "batch:   6%|▌         | 1500/24032 [04:37<1:06:33,  5.64it/s]\u001b[A\n",
      "batch:   7%|▋         | 1600/24032 [04:54<1:05:20,  5.72it/s]\u001b[A\n",
      "batch:   7%|▋         | 1700/24032 [05:12<1:05:23,  5.69it/s]\u001b[A\n",
      "batch:   7%|▋         | 1800/24032 [05:30<1:05:17,  5.67it/s]\u001b[A\n",
      "batch:   8%|▊         | 1900/24032 [05:47<1:04:42,  5.70it/s]\u001b[A\n",
      "batch:   8%|▊         | 2000/24032 [06:05<1:05:15,  5.63it/s]\u001b[A\n",
      "batch:   9%|▊         | 2100/24032 [06:23<1:04:57,  5.63it/s]\u001b[A\n",
      "batch:   9%|▉         | 2200/24032 [06:41<1:04:55,  5.60it/s]\u001b[A\n",
      "batch:  10%|▉         | 2300/24032 [06:59<1:04:52,  5.58it/s]\u001b[A\n",
      "batch:  10%|▉         | 2400/24032 [07:18<1:05:24,  5.51it/s]\u001b[A\n",
      "batch:  10%|█         | 2500/24032 [07:36<1:05:05,  5.51it/s]\u001b[A\n",
      "batch:  11%|█         | 2600/24032 [07:54<1:04:05,  5.57it/s]\u001b[A\n",
      "batch:  11%|█         | 2700/24032 [08:11<1:03:01,  5.64it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2800/24032 [08:28<1:02:02,  5.70it/s]\u001b[A\n",
      "batch:  12%|█▏        | 2900/24032 [08:45<1:01:13,  5.75it/s]\u001b[A\n",
      "batch:  12%|█▏        | 3000/24032 [09:02<1:00:53,  5.76it/s]\u001b[A\n",
      "batch:  13%|█▎        | 3100/24032 [09:20<1:00:56,  5.72it/s]\u001b[A\n",
      "batch:  13%|█▎        | 3200/24032 [09:37<1:00:29,  5.74it/s]\u001b[A\n",
      "batch:  14%|█▎        | 3300/24032 [09:56<1:01:22,  5.63it/s]\u001b[A\n",
      "batch:  14%|█▍        | 3400/24032 [10:16<1:03:09,  5.44it/s]\u001b[A\n",
      "batch:  15%|█▍        | 3500/24032 [10:33<1:02:07,  5.51it/s]\u001b[A\n",
      "batch:  15%|█▍        | 3600/24032 [10:51<1:01:25,  5.54it/s]\u001b[A\n",
      "batch:  15%|█▌        | 3700/24032 [11:08<1:00:26,  5.61it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    history = model.train(data_arr=params,\n",
    "                      logdir = \"../storage/Inverse/trainlogs/\", \n",
    "                      path_to_save='../storage/Inverse/trained_models/ind_mlp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-accountability",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FullModel:\n\tUnexpected key(s) in state_dict: \"top.task_layers.0.1.FC_layers.3.linear.weight\", \"top.task_layers.0.1.FC_layers.3.linear.bias\", \"top.task_layers.0.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.0.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.0.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.0.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.0.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.1.1.FC_layers.3.linear.weight\", \"top.task_layers.1.1.FC_layers.3.linear.bias\", \"top.task_layers.1.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.1.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.1.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.1.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.1.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.2.1.FC_layers.3.linear.weight\", \"top.task_layers.2.1.FC_layers.3.linear.bias\", \"top.task_layers.2.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.2.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.2.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.2.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.2.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.3.1.FC_layers.3.linear.weight\", \"top.task_layers.3.1.FC_layers.3.linear.bias\", \"top.task_layers.3.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.3.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.3.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.3.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.3.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.4.1.FC_layers.3.linear.weight\", \"top.task_layers.4.1.FC_layers.3.linear.bias\", \"top.task_layers.4.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.4.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.4.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.4.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.4.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.5.1.FC_layers.3.linear.weight\", \"top.task_layers.5.1.FC_layers.3.linear.bias\", \"top.task_layers.5.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.5.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.5.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.5.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.5.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.6.1.FC_layers.3.linear.weight\", \"top.task_layers.6.1.FC_layers.3.linear.bias\", \"top.task_layers.6.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.6.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.6.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.6.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.6.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.7.1.FC_layers.3.linear.weight\", \"top.task_layers.7.1.FC_layers.3.linear.bias\", \"top.task_layers.7.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.7.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.7.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.7.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.7.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.8.1.FC_layers.3.linear.weight\", \"top.task_layers.8.1.FC_layers.3.linear.bias\", \"top.task_layers.8.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.8.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.8.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.8.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.8.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.9.1.FC_layers.3.linear.weight\", \"top.task_layers.9.1.FC_layers.3.linear.bias\", \"top.task_layers.9.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.9.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.9.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.9.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.9.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.10.1.FC_layers.3.linear.weight\", \"top.task_layers.10.1.FC_layers.3.linear.bias\", \"top.task_layers.10.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.10.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.10.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.10.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.10.1.FC_layers.3.batch_normx.num_batches_tracked\". \n\tsize mismatch for top.task_layers.0.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-574642c1a269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mCTRAIN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mCTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     model.continue_training('../storage/Inverse/trained_models/ind_mlp_5ep.pt', data_arr = params,\n\u001b[0m\u001b[1;32m      4\u001b[0m                       \u001b[0mlogdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/data/workspace_files/trainlogs/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       path_to_save='/data/workspace_files/trained_models/ind_mlp_5ep.pt')\n",
      "\u001b[0;32m/notebooks/InverseProblem/inverse_problem/nn_inversion/main.py\u001b[0m in \u001b[0;36mcontinue_training\u001b[0;34m(self, checkpoint_path, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m    247\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1216\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FullModel:\n\tUnexpected key(s) in state_dict: \"top.task_layers.0.1.FC_layers.3.linear.weight\", \"top.task_layers.0.1.FC_layers.3.linear.bias\", \"top.task_layers.0.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.0.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.0.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.0.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.0.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.1.1.FC_layers.3.linear.weight\", \"top.task_layers.1.1.FC_layers.3.linear.bias\", \"top.task_layers.1.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.1.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.1.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.1.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.1.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.2.1.FC_layers.3.linear.weight\", \"top.task_layers.2.1.FC_layers.3.linear.bias\", \"top.task_layers.2.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.2.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.2.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.2.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.2.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.3.1.FC_layers.3.linear.weight\", \"top.task_layers.3.1.FC_layers.3.linear.bias\", \"top.task_layers.3.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.3.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.3.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.3.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.3.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.4.1.FC_layers.3.linear.weight\", \"top.task_layers.4.1.FC_layers.3.linear.bias\", \"top.task_layers.4.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.4.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.4.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.4.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.4.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.5.1.FC_layers.3.linear.weight\", \"top.task_layers.5.1.FC_layers.3.linear.bias\", \"top.task_layers.5.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.5.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.5.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.5.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.5.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.6.1.FC_layers.3.linear.weight\", \"top.task_layers.6.1.FC_layers.3.linear.bias\", \"top.task_layers.6.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.6.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.6.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.6.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.6.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.7.1.FC_layers.3.linear.weight\", \"top.task_layers.7.1.FC_layers.3.linear.bias\", \"top.task_layers.7.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.7.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.7.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.7.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.7.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.8.1.FC_layers.3.linear.weight\", \"top.task_layers.8.1.FC_layers.3.linear.bias\", \"top.task_layers.8.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.8.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.8.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.8.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.8.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.9.1.FC_layers.3.linear.weight\", \"top.task_layers.9.1.FC_layers.3.linear.bias\", \"top.task_layers.9.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.9.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.9.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.9.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.9.1.FC_layers.3.batch_normx.num_batches_tracked\", \"top.task_layers.10.1.FC_layers.3.linear.weight\", \"top.task_layers.10.1.FC_layers.3.linear.bias\", \"top.task_layers.10.1.FC_layers.3.batch_normx.weight\", \"top.task_layers.10.1.FC_layers.3.batch_normx.bias\", \"top.task_layers.10.1.FC_layers.3.batch_normx.running_mean\", \"top.task_layers.10.1.FC_layers.3.batch_normx.running_var\", \"top.task_layers.10.1.FC_layers.3.batch_normx.num_batches_tracked\". \n\tsize mismatch for top.task_layers.0.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.0.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.1.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.2.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.3.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.4.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.5.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.6.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.7.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.8.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.9.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.linear.weight: copying a param with shape torch.Size([16, 25]) from checkpoint, the shape in current model is torch.Size([1, 25]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.linear.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.batch_normx.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.batch_normx.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.batch_normx.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for top.task_layers.10.1.FC_layers.2.batch_normx.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "source": [
    "CTRAIN = True\n",
    "if CTRAIN:\n",
    "    model.continue_training('../storage/Inverse/trained_models/ind_mlp.pt', data_arr = params,\n",
    "                      logdir = \"/data/workspace_files/trainlogs/\", \n",
    "                      path_to_save='/data/workspace_files/trained_models/ind_mlp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-clearing",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-break",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind_mlp.pt  ind_mlp1.pt  ind_mlp_5ep.pt\n"
     ]
    }
   ],
   "source": [
    "!ls ../storage/Inverse/trained_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-bolivia",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss in epoch 0:  0.2949, val_loss: 0.0090\n",
      "Train loss in epoch 1:  0.0068, val_loss: 0.0064\n",
      "Train loss in epoch 2:  0.0063, val_loss: 0.0062\n",
      "Train loss in epoch 3:  0.0060, val_loss: 0.0058\n",
      "Train loss in epoch 4:  0.0058, val_loss: 0.0057\n"
     ]
    }
   ],
   "source": [
    "!cat ../storage/Inverse/trainlogs/history_hps_independent_mlp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-hayes",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../storage/Inverse/trained_models/ind_mlp_5ep.pt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile('ind_mlp_5ep.pt', '../storage/Inverse/trained_models/ind_mlp_5ep.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-snapshot",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6956221"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path('InverseProblem/res_experiments/ind_mlp.pt').stat().st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-reproduction",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "model.load_model('../storage/Inverse/trained_models/ind_mlp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-bulgaria",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "predicted, params = model.predict_full_image('../storage/dataset/20170905_030404.fits', '../storage/dataset/prediction_ind.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-appearance",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from inverse_problem.nn_inversion.posthoc import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-report",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 0.42372166014440654\n",
      "rmse 0.0911952627108014\n",
      "mse 0.008519699465119986\n",
      "mae 0.06686942715241348\n",
      "\n",
      "r2 0.855972235076514\n",
      "rmse 0.054889532469593216\n",
      "mse 0.003127715710796275\n",
      "mae 0.030736477790489745\n",
      "\n",
      "r2 0.5587789320320499\n",
      "rmse 0.17296730877987865\n",
      "mse 0.030887749262352104\n",
      "mae 0.09180101937208447\n",
      "\n",
      "r2 0.6140851701066813\n",
      "rmse 0.07980404849682887\n",
      "mse 0.006412706393531415\n",
      "mae 0.0657734480015369\n",
      "\n",
      "r2 0.0873013793439008\n",
      "rmse 0.13411552594006676\n",
      "mse 0.01819365378582341\n",
      "mae 0.10352890724162818\n",
      "\n",
      "r2 -27.036770586411897\n",
      "rmse 0.6880629931003045\n",
      "mse 0.47992310614300615\n",
      "mae 0.5794515163548253\n",
      "\n",
      "r2 -9.34840380303171\n",
      "rmse 0.20125868294576976\n",
      "mse 0.04062056678320126\n",
      "mae 0.19181903660457705\n",
      "\n",
      "r2 -0.1619970456676841\n",
      "rmse 0.10007970749489226\n",
      "mse 0.010085058920222576\n",
      "mae 0.0905935859585546\n",
      "\n",
      "r2 0.830677212992509\n",
      "rmse 0.018941086751164044\n",
      "mse 0.00037845649714565065\n",
      "mae 0.011180359168162991\n",
      "\n",
      "r2 -0.49806253436553194\n",
      "rmse 0.2935490009162807\n",
      "mse 0.08723083406148262\n",
      "mae 0.22942011330860237\n",
      "\n",
      "r2 0.0647166513061676\n",
      "rmse 0.07806657249858077\n",
      "mse 0.006304958706706321\n",
      "mae 0.04256958294445926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_score = []\n",
    "for i in range(11):\n",
    "    param_score.append(metrics(params[:, :, i], predicted[:, :, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-translation",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
